@misc{oordConditional2016,
    title = {Conditional {Image} {Generation} with {PixelCNN} {Decoders}},
    url = {http://arxiv.org/abs/1606.05328},
    doi = {10.48550/arXiv.1606.05328},
    abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
    urldate = {2025-08-13},
    publisher = {arXiv},
    author = {Oord, Aaron van den and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
    month = jun,
    year = {2016},
    note = {arXiv:1606.05328 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, first pass, second pass, third pass},
    file = {Preprint PDF:/Users/nico/Zotero/storage/PAIMI8FJ/Oord et al. - 2016 - Conditional Image Generation with PixelCNN Decoders.pdf:application/pdf;Snapshot:/Users/nico/Zotero/storage/6LRXISB3/1606.html:text/html},
}


@misc{oordNeural2018,
    title = {Neural {Discrete} {Representation} {Learning}},
    url = {http://arxiv.org/abs/1711.00937},
    doi = {10.48550/arXiv.1711.00937},
    abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
    urldate = {2025-05-21},
    publisher = {arXiv},
    author = {Oord, Aaron van den and Vinyals, Oriol and Kavukcuoglu, Koray},
    month = may,
    year = {2018},
    note = {arXiv:1711.00937 [cs]},
    keywords = {Computer Science - Machine Learning, first pass, second pass, third pass},
    annote = {VQ VAE paper
},
    file = {Preprint PDF:/Users/nico/Zotero/storage/XTRAQUZR/Oord et al. - 2018 - Neural Discrete Representation Learning.pdf:application/pdf;Snapshot:/Users/nico/Zotero/storage/IFXEJMWI/1711.html:text/html},
}


@article{pinayaUnsupervised2022,
    title = {Unsupervised brain imaging {3D} anomaly detection and segmentation with transformers},
    volume = {79},
    issn = {1361-8415},
    url = {https://www.sciencedirect.com/science/article/pii/S1361841522001220},
    doi = {10.1016/j.media.2022.102475},
    abstract = {Pathological brain appearances may be so heterogeneous as to be intelligible only as anomalies, defined by their deviation from normality rather than any specific set of pathological features. Amongst the hardest tasks in medical imaging, detecting such anomalies requires models of the normal brain that combine compactness with the expressivity of the complex, long-range interactions that characterise its structural organisation. These are requirements transformers have arguably greater potential to satisfy than other current candidate architectures, but their application has been inhibited by their demands on data and computational resources. Here we combine the latent representation of vector quantised variational autoencoders with an ensemble of autoregressive transformers to enable unsupervised anomaly detection and segmentation defined by deviation from healthy brain imaging data, achievable at low computational cost, within relative modest data regimes. We compare our method to current state-of-the-art approaches across a series of experiments with 2D and 3D data involving synthetic and real pathological lesions. On real lesions, we train our models on 15,000 radiologically normal participants from UK Biobank and evaluate performance on four different brain MR datasets with small vessel disease, demyelinating lesions, and tumours. We demonstrate superior anomaly detection performance both image-wise and pixel/voxel-wise, achievable without post-processing. These results draw attention to the potential of transformers in this most challenging of imaging tasks.},
    urldate = {2025-08-11},
    journal = {Medical Image Analysis},
    author = {Pinaya, Walter H. L. and Tudosiu, Petru-Daniel and Gray, Robert and Rees, Geraint and Nachev, Parashkev and Ourselin, Sebastien and Cardoso, M. Jorge},
    month = jul,
    year = {2022},
    keywords = {first pass, second pass, Anomaly detection, Transformer, Unsupervised anomaly segmentation, Vector quantized variational autoencoder},
    pages = {102475},
    file = {ScienceDirect Full Text PDF:/Users/nico/Zotero/storage/Y8KDKFMM/Pinaya et al. - 2022 - Unsupervised brain imaging 3D anomaly detection and segmentation with transformers.pdf:application/pdf;ScienceDirect Snapshot:/Users/nico/Zotero/storage/5MZ9H24U/S1361841522001220.html:text/html},
}


@article{fraunhoferinstituteforintegratedcircuitsiisTwostages2023,
    title = {A two-stages {Simulation}-based {Neural} {Network} for {Defect} {Detection} on {3D} {CT} {Data}},
    volume = {1},
    issn = {2941-4989},
    url = {https://www.ndt.net/search/docs.php3?id=28146},
    doi = {10.58286/28146},
    abstract = {Computed tomography (CT) is a prominent technology for nondestructive quality control and is already used in industry for defect detection. However, as quality control is shifting towards a full in-line inspection, automatic CT analysis is required to meet the tight production time. Nonetheless, in settings where a high amount of data is produced, a robust fully automatic defect detection is essential In the past years, deep learning (DL) has been extensively used to perform vision tasks in an automatic way, and given its promising results, has been successfully applied in CT settings. Most of the recent work is based on supervised DL often adapted from results in the medical field. Supervised DL, although extremely powerful, has the drawbacks of requiring a high amount of labeled data done by experts and is biased to the specific dataset used. Therefore, an unsupervised DL model is presented. A two stages network formed by an auto-encoder and an autoregressive model, originally implemented for image generation, is adapted for volume segmentation. The network is trained on the specific task of defect segmentation of cast aluminum parts. CAD models of such parts are gathered, and corresponding simulated CT scans are acquired. Results show that the architecture, although originally implemented for data generation, can be adapted for CT volume segmentation.},
    language = {en},
    number = {1},
    urldate = {2025-05-13},
    journal = {Research and Review Journal of Nondestructive Testing},
    author = {Florian, Virginia and Kretzer, Christian and Kasperl, Stefan and Schielein, Richard and Montavon, Benjamin and Schmitt, Robert H.},
    month = aug,
    year = {2023},
    keywords = {first pass, second pass, third pass},
    file = {PDF:/Users/nico/Zotero/storage/G4WEHP3P/Fraunhofer Institute for Integrated Circuits (IIS) et al. - 2023 - Unsupervised deep learning for defect detection on CT parts using simulated data.pdf:application/pdf},
}

@article{PYRONN2019,
author = {Syben, Christopher and Michen, Markus and Stimpel, Bernhard and Seitz, Stephan and Ploner, Stefan and Maier, Andreas K.},
title = {Technical Note: PYRO-NN: Python reconstruction operators in neural networks},
year = {2019},
journal = {Medical Physics},
}


@incollection{paschali3DQ2019,
    title = {{3DQ}: {Compact} {Quantized} {Neural} {Networks} for {Volumetric} {Whole} {Brain} {Segmentation}},
    volume = {11766},
    shorttitle = {{3DQ}},
    url = {http://arxiv.org/abs/1904.03110},
    abstract = {Model architectures have been dramatically increasing in size, improving performance at the cost of resource requirements. In this paper we propose 3DQ, a ternary quantization method, applied for the first time to 3D Fully Convolutional Neural Networks (F-CNNs), enabling 16x model compression while maintaining performance on par with full precision models. We extensively evaluate 3DQ on two datasets for the challenging task of whole brain segmentation. Additionally, we showcase our method's ability to generalize on two common 3D architectures, namely 3D U-Net and V-Net. Outperforming a variety of baselines, the proposed method is capable of compressing large 3D models to a few MBytes, alleviating the storage needs in space critical applications.},
    urldate = {2025-05-09},
    author = {Paschali, Magdalini and Gasperini, Stefano and Roy, Abhijit Guha and Fang, Michael Y.-S. and Navab, Nassir},
    year = {2019},
    doi = {10.1007/978-3-030-32248-9_49},
    note = {arXiv:1904.03110 [cs]},
    keywords = {Computer Science - Computer Vision and Pattern Recognition},
    pages = {438--446},
    annote = {Comment: Accepted to MICCAI 2019},
    file = {Preprint PDF:/Users/nico/Zotero/storage/XP6MSF3S/Paschali et al. - 2019 - 3DQ Compact Quantized Neural Networks for Volumetric Whole Brain Segmentation.pdf:application/pdf;Snapshot:/Users/nico/Zotero/storage/RR3MUDKK/1904.html:text/html},
}


@article{suiPruning2025,
    title = {Pruning {3D} {Convolutional} {Neural} {Networks} via {Channel} {Independence}},
    issn = {1939-8115},
    url = {https://doi.org/10.1007/s11265-025-01950-1},
    doi = {10.1007/s11265-025-01950-1},
    abstract = {Network pruning provides a promising approach for deploying costly Deep Neural Network (DNN) models on resource-constrained devices. However, most existing pruning works focus on compressing traditional convolutional DNN models for image-related classification and object-detection tasks in 2D scenarios. Due to the complicated structure of 3D CNN models, pruning such models has not been well studied. In this paper, we analyze the different properties between 2D and 3D tasks, then propose a Filter/Depth-wise Independence Score (FDIS) to evaluate the importance of each filter. In addition, we adopt several granularity schemes to improve the performance of the proposed method. To achieve fine-grained pruning, we prune the networks gradually using an iterative pruning procedure. In addition, we experimentally show that weights with low independence scores contain less important information, enabling the removal of filters without serious accuracy degradation. Our proposed FDIS-based approach maintains high accuracy with certain FLOP reduction and practical acceleration.},
    language = {en},
    urldate = {2025-09-01},
    journal = {Journal of Signal Processing Systems},
    author = {Sui, Yang and Anjum, Khizar and Pompili, Dario and Yuan, Bo},
    month = aug,
    year = {2025},
    keywords = {3D Neural networks, Pruning, Video recognition},
    file = {Full Text PDF:/Users/nico/Zotero/storage/J7IJ94Z7/Sui et al. - 2025 - Pruning 3D Convolutional Neural Networks via Channel Independence.pdf:application/pdf},
}

@misc{weiler3D2018,
    title = {{3D} {Steerable} {CNNs}: {Learning} {Rotationally} {Equivariant} {Features} in {Volumetric} {Data}},
    shorttitle = {{3D} {Steerable} {CNNs}},
    url = {http://arxiv.org/abs/1807.02547},
    doi = {10.48550/arXiv.1807.02547},
    abstract = {We present a convolutional network that is equivariant to rigid body motions. The model uses scalar-, vector-, and tensor fields over 3D Euclidean space to represent data, and equivariant convolutions to map between such representations. These SE(3)-equivariant convolutions utilize kernels which are parameterized as a linear combination of a complete steerable kernel basis, which is derived analytically in this paper. We prove that equivariant convolutions are the most general equivariant linear maps between fields over R{\textasciicircum}3. Our experimental results confirm the effectiveness of 3D Steerable CNNs for the problem of amino acid propensity prediction and protein structure classification, both of which have inherent SE(3) symmetry.},
    urldate = {2025-09-01},
    publisher = {arXiv},
    author = {Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco},
    month = oct,
    year = {2018},
    note = {arXiv:1807.02547 [cs]},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
    file = {Preprint PDF:/Users/nico/Zotero/storage/EQYT9V3H/Weiler et al. - 2018 - 3D Steerable CNNs Learning Rotationally Equivariant Features in Volumetric Data.pdf:application/pdf;Snapshot:/Users/nico/Zotero/storage/RNIW2VKE/1807.html:text/html},
}


@misc{geigerE3nn2022,
    title = {e3nn: {Euclidean} {Neural} {Networks}},
    shorttitle = {e3nn},
    url = {http://arxiv.org/abs/2207.09453},
    doi = {10.48550/arXiv.2207.09453},
    abstract = {We present e3nn, a generalized framework for creating E(3) equivariant trainable functions, also known as Euclidean neural networks. e3nn naturally operates on geometry and geometric tensors that describe systems in 3D and transform predictably under a change of coordinate system. The core of e3nn are equivariant operations such as the TensorProduct class or the spherical harmonics functions that can be composed to create more complex modules such as convolutions and attention mechanisms. These core operations of e3nn can be used to efficiently articulate Tensor Field Networks, 3D Steerable CNNs, Clebsch-Gordan Networks, SE(3) Transformers and other E(3) equivariant networks.},
    urldate = {2025-09-01},
    publisher = {arXiv},
    author = {Geiger, Mario and Smidt, Tess},
    month = jul,
    year = {2022},
    note = {arXiv:2207.09453 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
    annote = {Comment: draft},
    file = {Preprint PDF:/Users/nico/Zotero/storage/VRACZ6H5/Geiger and Smidt - 2022 - e3nn Euclidean Neural Networks.pdf:application/pdf;Snapshot:/Users/nico/Zotero/storage/4MZ25E6P/2207.html:text/html},
}

@misc{angelopoulosGentle2022,
    title = {A {Gentle} {Introduction} to {Conformal} {Prediction} and {Distribution}-{Free} {Uncertainty} {Quantification}},
    url = {http://arxiv.org/abs/2107.07511},
    doi = {10.48550/arXiv.2107.07511},
    abstract = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90\%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed to provide the reader a working understanding of conformal prediction and related distribution-free uncertainty quantification techniques with one self-contained document. We lead the reader through practical theory for and examples of conformal prediction and describe its extensions to complex machine learning tasks involving structured outputs, distribution shift, time-series, outliers, models that abstain, and more. Throughout, there are many explanatory illustrations, examples, and code samples in Python. With each code sample comes a Jupyter notebook implementing the method on a real-data example; the notebooks can be accessed and easily run using our codebase.},
    urldate = {2025-08-11},
    publisher = {arXiv},
    author = {Angelopoulos, Anastasios N. and Bates, Stephen},
    month = dec,
    year = {2022},
    note = {arXiv:2107.07511 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology, Statistics - Statistics Theory},
    annote = {Comment: Blog and tutorial video at http://angelopoulos.ai/blog/posts/gentle-intro/ ; Code is available at https://github.com/aangelopoulos/conformal-prediction},
    file = {Preprint PDF:/Users/nico/Zotero/storage/ER2DM89R/Angelopoulos and Bates - 2022 - A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification.pdf:application/pdf;Snapshot:/Users/nico/Zotero/storage/44SFLRDV/2107.html:text/html},
}

@misc{cohenCertified2019,
    title = {Certified {Adversarial} {Robustness} via {Randomized} {Smoothing}},
    url = {http://arxiv.org/abs/1902.02918},
    doi = {10.48550/arXiv.1902.02918},
    abstract = {We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the \${\textbackslash}ell\_2\$ norm. This "randomized smoothing" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in \${\textbackslash}ell\_2\$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49\% under adversarial perturbations with \${\textbackslash}ell\_2\$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified \${\textbackslash}ell\_2\$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at http://github.com/locuslab/smoothing.},
    urldate = {2025-09-01},
    publisher = {arXiv},
    author = {Cohen, Jeremy M. and Rosenfeld, Elan and Kolter, J. Zico},
    month = jun,
    year = {2019},
    note = {arXiv:1902.02918 [cs]},
    keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
    annote = {Comment: ICML 2019},
    file = {Preprint PDF:/Users/nico/Zotero/storage/V9CC9FS9/Cohen et al. - 2019 - Certified Adversarial Robustness via Randomized Smoothing.pdf:application/pdf;Snapshot:/Users/nico/Zotero/storage/DT46ELP8/1902.html:text/html},
}

